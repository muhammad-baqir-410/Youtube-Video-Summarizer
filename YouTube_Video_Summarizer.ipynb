{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMKviWWQFMb7"
      },
      "source": [
        "## **Use LangChain and ChatGPT to Summarize YouTube Videos of Any Length**\n",
        "\n",
        "\n",
        "\n",
        "This notebook shows all the steps to use LangChain and OpenAI's GPT 3.5 to create summaries of YouTube videos.\n",
        "\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "\n",
        "We'll be coveringt the following steps in this tutorial:\n",
        "\n",
        "1. Installing Dependencies\n",
        "2. Define helper functions to extract transcripts from YouTube videos\n",
        "3. Convert the text into a doc using LangChain\n",
        "4. Split the document into chunks using LangChain\n",
        "5. Create a summary using ChatGPT + LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4KVCTQ9V0bU"
      },
      "source": [
        "## 1. **Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCIeZROjtgCk",
        "outputId": "216ba606-2e5c-442e-c946-a64d24331038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m454.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m811.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.20\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.8/282.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.3/154.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: urllib3, tqdm, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 frozenlist-1.3.3 idna-3.4 multidict-6.0.4 openai-0.27.7 requests-2.31.0 tqdm-4.65.0 urllib3-2.0.2 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmxVgr9JLTin",
        "outputId": "101e7f68-a4c6-4815-bdf9-a1138ff15e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.179-py3-none-any.whl (907 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m907.7/907.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML>=5.4.1\n",
            "  Using cached PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4\n",
            "  Downloading SQLAlchemy-2.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from langchain) (3.8.4)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Using cached dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting numexpr<3.0.0,>=2.8.4\n",
            "  Downloading numexpr-2.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m642.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m768.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2,>=1\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1\n",
            "  Downloading pydantic-1.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Collecting tenacity<9.0.0,>=8.1.0\n",
            "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from pydantic<2,>=1->langchain) (4.6.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Collecting greenlet!=0.4.17\n",
            "  Downloading greenlet-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (618 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.8/618.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, PyYAML, pydantic, numpy, mypy-extensions, marshmallow, greenlet, typing-inspect, SQLAlchemy, openapi-schema-pydantic, numexpr, marshmallow-enum, dataclasses-json, langchain\n",
            "Successfully installed PyYAML-6.0 SQLAlchemy-2.0.15 dataclasses-json-0.5.7 greenlet-2.0.2 langchain-0.0.179 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 numexpr-2.8.4 numpy-1.24.3 openapi-schema-pydantic-1.2.4 pydantic-1.10.8 tenacity-8.2.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eTMG28A5zua",
        "outputId": "79efd1e3-a64e-4c3f-9592-1c33b5b37d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests->youtube-transcript-api) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests->youtube-transcript-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests->youtube-transcript-api) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests->youtube-transcript-api) (2023.5.7)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api # for Linux and MacOs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpRJ0po_vSMW",
        "outputId": "87df655e-592c-4afd-ecd7-836817455bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m445.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2022.1.18\n",
            "  Downloading regex-2023.5.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m244.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahmad/anaconda3/envs/YT_summary/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Installing collected packages: regex, tiktoken\n",
            "Successfully installed regex-2023.5.5 tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjgkCMq9WLGe"
      },
      "source": [
        "## **2. Add Video URL**\n",
        "Insert the URL of the video you want to summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VJlCzCjJvuWa"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.youtube.com/watch?v=LWiM-LuRe6w&t=1204s' ## Replace this with the URL of video you want to summarize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o0dizwTWccM"
      },
      "source": [
        "## **3. Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AU-MHi-q5sT5"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "OPENAI_KEY = \"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\" ## Add your API key \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwYAh55JWprB"
      },
      "source": [
        "## **4. Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aY386t9NW3ob"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_youtube_id(url):\n",
        "    youtube_id_match = re.search(r'(?<=v=)[^&#]+', url)\n",
        "    youtube_id_match = youtube_id_match or re.search(r'(?<=be/)[^&#]+', url)\n",
        "    trailer = youtube_id_match.group(0) if youtube_id_match else None\n",
        "    return trailer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U4jwhGB75uiq"
      },
      "outputs": [],
      "source": [
        "video_id = extract_youtube_id(url)\n",
        "srt = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "text_arr=''\n",
        "\n",
        "for ele in srt:\n",
        "  text_arr=text_arr+' '+ele['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "avEFqAQRzAI3"
      },
      "outputs": [],
      "source": [
        "def text_to_doc(text_arr):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "  text = [text_arr]\n",
        "  page_docs = [Document(page_content=page) for page in text]\n",
        "\n",
        "  # Add page numbers as metadata\n",
        "  for i, doc in enumerate(page_docs):\n",
        "      doc.metadata[\"page\"] = i + 1\n",
        "\n",
        "  # Split pages into chunks\n",
        "  doc_chunks = []\n",
        "\n",
        "  for doc in page_docs:\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=800,\n",
        "          separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        "          chunk_overlap=0,\n",
        "      )\n",
        "      chunks = text_splitter.split_text(doc.page_content)\n",
        "      for i, chunk in enumerate(chunks):\n",
        "          doc = Document(\n",
        "              page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
        "          )\n",
        "          # Add sources a metadata\n",
        "          doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
        "          doc_chunks.append(doc)\n",
        "  return doc_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy5F1R1hXugf"
      },
      "source": [
        "## **5. Code to generate summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uXMkOl13Kk28"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"The following is a portion of a transcript from a \n",
        "youtube video. Your job is to write a concise summary.\n",
        "\n",
        "{text}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pr1Z7ipEsqby"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "model_name='gpt-4'\n",
        "\n",
        "model_name='gpt-3.5-turbo'\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name=model_name,temperature=0.3,openai_api_key=OPENAI_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "omr3iiuLywqS"
      },
      "outputs": [],
      "source": [
        "doc_chunks=text_to_doc(text_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U3Fw5ogdtdVC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3d11ed938d155603c8dd7ef65ecb0f57 in your message.).\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4f4ca047c029269195bb3ece3af9b2dd in your message.).\n"
          ]
        }
      ],
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\",map_prompt=PROMPT, combine_prompt=PROMPT)\n",
        "summary = chain.run(doc_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USNfYsDbfIkj"
      },
      "source": [
        "## **6. Summary Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "3oDSWjclt7cC",
        "outputId": "2bbf4712-997c-4779-d2ea-b78182939427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The video discusses the potential dangers of artificial intelligence (AI) and the need for regulation to prevent it from causing harm to society. The speaker notes that AI has the potential to manipulate human language and create a matrix-like world of illusions, leading to societal polarization, undermined mental health, and destabilized democratic societies. The video also touches on the controversy surrounding online censorship and the difficulty of regulating AI due to the amount of computing power and money required. The speaker emphasizes the need for time to understand what kind of regulations are necessary and suggests that the first regulation should be to make it mandatory for AI to disclose that it is an AI.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Xga_q7i405SG"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_KEY)\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to write a detailed linkedin article using the given summary of a selected topic\n",
        "\n",
        "{context}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbj6pXZ2fpyu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
        "\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0,openai_api_key=OPENAI_KEY), \n",
        "    prompt=PROMPT, \n",
        "    verbose=True, \n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to write a detailed linkedin article using the given summary of a selected topic\n",
            "\n",
            "The video discusses the potential dangers of artificial intelligence (AI) and the need for regulation to prevent it from causing harm to society. The speaker notes that AI has the potential to manipulate human language and create a matrix-like world of illusions, leading to societal polarization, undermined mental health, and destabilized democratic societies. The video also touches on the controversy surrounding online censorship and the difficulty of regulating AI due to the amount of computing power and money required. The speaker emphasizes the need for time to understand what kind of regulations are necessary and suggests that the first regulation should be to make it mandatory for AI to disclose that it is an AI.\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "output = chatgpt_chain.predict(context=summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary: This video discusses the potential dangers of artificial intelligence (AI) and the need for regulation to prevent it from causing harm to society.\n",
            "\n",
            "In a recent video, the speaker discussed the potential dangers of artificial intelligence (AI) and the need for regulation to prevent it from causing harm to society. AI has the potential to manipulate human language and create a matrix-like world of illusions, leading to societal polarization, undermined mental health, and destabilized democratic societies. The speaker also touched on the controversy surrounding online censorship and the difficulty of regulating AI due to the amount of computing power and money required.\n",
            "\n",
            "The speaker emphasized the need for time to understand what kind of regulations are necessary and suggested that the first regulation should be to make it mandatory for AI to disclose that it is an AI. This would help to ensure that people are aware of the potential dangers of AI and can make informed decisions about how to interact with it.\n",
            "\n",
            "The potential dangers of AI are real and must be addressed. It is essential that we take the time to understand the implications of AI and create regulations that will protect society from its potential harms. We must also ensure that AI is transparent and that people are aware of its capabilities. Only then can we ensure that AI is used responsibly and\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
